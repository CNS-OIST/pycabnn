###### How to use scheme

- put everything into brackets
- prefix language -> first one is the operator, can use as many arguments as you want:  (+ 2 4 6)
- correspondingly (f x y) correspondes to f(x,y)
- if I want to just print digits, I need to quote: (quote(2 3 4)), short '(2 3 4)

Lists:
- my list is '(1 2 3)
- I can access single elements from it like this:
    (car '(1 2 3)) -> first element = 1
    (cdr '(1 2 3)) -> tail (all but first) = 2 3
- if I want elements that are further behind, I can just stack the comands, i.e.: (car (cdr ( '(1 2 3))) = 2 -> (cadr '(1 2 3))
    Third element: (caddr( '(1 2 3)) = 3
- Add single element to ffront of list: cons (tail needs to be empty list '() if it is the first element
    (cons 1 (cons 2 '()) -> ( 1 2)
- Faster: (define lst '( 1 2 3 4))
- Also possible: (list 1 2 3 4) -> but then there are restrictions on what you can put after the list keyword) -> no nested definition possible at that moment.


Lambdas
(define incr(lambda (x) (+ x 1)))
(define incr(lambda (x) x 3(+ x 1))) -> its the same. Ignores all the useless stuff and only takes the last statement for evaluation.
Scheme: recursion!


Whitespaces:
New tokens start with a whitespace
So you can put pretty weird stuff in a name
(define a+b (lambda (a b) (+ a b))) 
thus you also usually don't capitalize, but rather use - (list-length rather than listLength)

Comments
Commenting character ;
Multiline comments with #| ... |#
Commenting a whole expression: ;# (def...)

######## Hints on how to translate from one pogramming language to another

Things to care for:
    - Programming idioms
    - Efficient data and memory usage


https://github.com/6502/pylisp
-> this guy wrote a isp dialerct compiler targeting Python bytecode


Could be useful:
https://github.com/hylang/hy.git
http://docs.hylang.org/en/stable/


http://symbolhound.com/ -> search engine that allows searching for special characters. Does not seem that great though.



######## Scheme programming
https://stackoverflow.com/questions/12183096/
(if test consequence alternative)
-> allows only one statement as consequence and one as alternative
Alternatives:
(cond ( test cons1 cons2 cons...)
(else alt1 alt2 ... ) )
Ór assembe the cosequences using a begin statement

(let kw-definitions functions-using-kwds) -> local definition of certain kwds

The random mtzig package generates different kinds of random numbers
To do so, it needs a generator state vector st. This vector is generated in the init function depending on a seed, i.e. (random-mtzig:init 12) is generating a state vector on the seed 12 that can then be used to generate random numbers.

Records: custom-made data types. Initialization:
(define-record-type point (fields x y))

I do not see them  used later on so far, but let's see.
We have the following records:
- pointset (make-pointset prefix id points boundary)
- bounds (make-bounds top left bottom right)
- genpoint (make-genpoint coords parent-index parent-distance segment)
- cell (make-cell ty index origin sections)

There are the following  x->kd-tree routines:
 - cells-sections->kd-tree
 - sections->kd-tree
 - cells-origins->kd-tree
-> it seems as if all of them stack information first in a list and then call the list->kd-tree ccommand from the kd-tree package

let* is a local binding just like let, but it evaluates from left to right. Thus, the declarations in the first pair are visible and can be used for the second pair.

and: evaluate everything behind the and until one expression gives a #f
or: evaluate everything until an expression gives something other than #f

recur: from https://www.informatik.uni-kiel.de//~scheme/doc/advanced/recur.html
(recur NAME ((variable expr) ...  ) expression) -> using the name of the expression, variables are defined in the second part, the last part shows what is done with the variables. 

filter: goes filter predicate list
-> only elements of list for which predicate is true will be returned.



######## And on the metalevel
From the Scheme Wiki: 
http://wiki.call-cc.org/man/4/Getting%20started
- R5RS = The Revised⁵ Report on Scheme -> standard definitions. There are newer revisions, but they are not as widely accepted.
- SRFI = Scheme Requests for Implementation -> Define new language features.
- Scheme is supposed to be a very minimalist language, so above extensions add the hopefully most useful features
- http://www.schemers.org -> recommended starting point for scheme knowledge icluding resources, reports, SRFIs,...

CHICKEN
- an implementation of scheme that supports almost all important SRFIs
- compiles to portable C code that supports a lot of stuff and offers a handy interface to and from C libraries. Also allows embedded C code.
- also includes and interpreter for interactive use
- framework for language extensions
- eggs = language extensions, include interfaces to other languages (Python included), GUIs, ...

Links
- http://www.call-cc.org -> Master chicken website
- http://wiki.call-cc.org/egg-index -> list of eggs
- http://api.call-cc.org -> questions about chicken
- http://bugs.call-cc.org -> bugs and issue tracker
- IRC channel (#chicken) on Freenode
- http://code.call-cc.org/ -> codebase, including installation instructions.

Interpreter:
- start with csi
- load files using (load "bla.scm")
- REPL = read eval print loop
- can be customized (http://wiki.call-cc.org/man/4/Using%20the%20interpreter) and used for debugging, e.g. by starting lines with a ,

Compiler:
- faster and more portable
- chicken is the command for the chicken compiler, but csc is preferred
    - it compiles Scheme -> C, C-> object code => link results in executable file.
- file.so (shared object) after compilation


More on the compilation
- separate compilation of modules is possible
- all three statical, dynamical linking and dynamic loading are supported
- unit = single compiled object module containing toplevel expressions
- Evaluation when the unit is the main unit or it is used (e.g. by the main unit)
- In order that a unit can be used it has to be declared by 
(declare (uses UNITNAME))
- compile file as unit: (declare (unit UNITNAME))
- Other option: (include "FILENAME") -> the code will just be inserted, thus not be compiled before the include statement
- macros (general mappings of long code blocks to short statements) thus ave to be processed by include or import


###### Wikipedia says about scheme:
    - functional programming language
    - one of the main dialects of Lisp, but more minimalist with options of expansion thoug
    - said to be the most unportable programming language and more a family of dialects than a language
    - 

functional programming language:
    - treats programs as evaluations of mathematical functions
    - avoids chaging-state and mutable data
    - declarative programming: if you have a function, it depends only on the arguments, which might be function values, but only get back to fixly declared variables. Local or global states do not go in.
    
Concepts:
    - First-class or higher-order functions:
        - take other functions as arguments or return them as results (e.g. differential operator)
        - First class = computer science term, means tat there are no restriction on the use of some entities of a programming language
        - Partial application and currying
    - Pure functions (expressions)
        - no side effects (memory or I/O)
        - can thus be used to optimize code
        - if there is no data dependency, pure expressions are thread-safe, compilers can arrange it in the most useful way
    - Recursion
        - Tail = subroutine call performed as the final action of a procedure.
        - Tail Recursion: 
        


###### Other insights

f2c: 
	- converts Fortran 77 to C 
	- relies on a C compiler backendand is therefore more portable than other compilers which generate machine code directly
	- free, infleunced e.g. the GNU g77 compiler, made it possible to have a lot of fortran libraries translated to C

machine code: Set of instructions that is directly executed by a computer's CPU
	- very specific tasks such as load, jump/branch (i.e. here, the program may jump to another sequence e.g. in a loop, i.e. reset the program counter of the CPU mechanically), or an ALU (arithmetic logic unit, performs arithmetic and bitwise operations on integers)
	-lowest programming detail available to the programmer, beyond this there might be microcode though.
	- Usually programming happens in a higher-level language which is then translated by a compiler, assembler or linker (except interpreted programs)

Interpreted programs:
	- here, the interpreter translates each statement into a sequence of one or more subroutines that are already compiled into machine code. 
	- Many languages use both (interpreter and compiler) though, e.g, Lisp, C, Python.

GCC (Gnu Compiler Collection):
	- includes compilers for several languages, runs under free license and is therefore a very important GNU programming tool (part of the GNU toolchain)

tarball: (tar = tape archive)
	- all kind of stuff assembled in an archive file that also contains some metadata
	- header in ASCII to ensure portability
	- .gz ending when it is compressed by gzip

SHA256sum:
	- cryptographic hash that can be used to confirm integrity and authenticity of a file
	- related to MD5, but apparantly newer and safer.
	
runtime = collection of software and hardware resources that are needed to execute a software program on a computer system

In file browser, ctrl + l gets you the pwd :)

tar -czvf name-of-archive.tar.gz /path/to/directory-or-file

ps -eo pcpu,pid,user,args | sort -r -k1 | less -> see current PCU processes and how much they use

####### NEURON

hoc files:
- based on the floating point calculator developed in 1984 'he Unix Programming Environment'
- C like syntax , very similar to the 'bc' calculator
- object oriented syntax addition -> implement abstract data types, data encapsulations, polymorphisms (no inheritance)
- neuron3 version of the interpreter is called oc
- $NEURONHOME/lib/help/oc.help contains synopsis of each command and function, ivoc.help  for graphical interface, nrnoc.help and rniv.help contain neuron specific syntax/functions
- HOC interpreter has served as general I/O module in dierse applications -> exec under different names
- the later the neuron version the more custom syntax available.
- In NEURON all hoc files can be accessed from python

NEURON GUI tools
- Channel Builder
- CellBuilder -> build mudels from scratch, modify existing models without writing any code
- Import3D can convert morphometric data from various formats into model cells
- Linear Circuit Builder -> set up models that involve gap juncions, ephaptic interactions, different clamp techniques, ...
- Network Builder, can be used to prototype small networks

Model Analysis and Optimization Tools:
- ModelView: discovers and represents model properties -> cann emit and import model specifications in XML, helps users understand each other's models -> facilitates codesharing. 
- Impedance Tool: electrotonic analyses of a model cell
- Multiple Run Fitter -> helps settinng up optimization protocols for automated tuning of model parameters





###### Cluster insights

edited the .ssh/config file
ssh sango -> connect to cluster
scp <file> sango: -> copy file ... to sango (default = homedir)
scp -r <dir> sango -> copy folder to sango
sftp sango -> can be used to browse sango. Usual commands such as ls, cd, .. possible. 
    put (upload file), get (download file) are additional options, didn't work with folders straight by just adding -r
despite editing the batch file, it did not work. Will investigate in the reasons, however here is the line again that allows copying of stuff into my directory on the sango cluster: (from local terminal)

scp brep_deploy.tar.gz ines-wichert@sango.oist.jp:/home/i/ines-wichert

For multithreading, the following 
https://groups.oist.jp/scs/paralell-jobs-sango


SLURM = Simple Linux Utility for Resource Management
- allocates access to 


cluster computers: 
- more or less tightly connected computers that work together
- each node performs the same task, controlled and scheduled by software
    -> vs grid computers: workload is not shared, each node may perform a different task, often the heterogeneity is higher
- node = computer used as a server, has own instance of operating system
- often all nodes use the same hardware, connection by fast local area network, but ther are ways to connect different hardware and/or operating systems (OSCAR = Open Source Cluster Application Resources)
- Load-Balancing: workload distribution among the nodes in order to optimize overall performance and response time
- High availability (failover, HA-clusters): redundant nodes and other components are used to back-up failure of others
- Message passing: so that the cluster nodes can communicate
    - PVM = Parallel Virtual Machine (older) -> concrete implementation
    - MPI = Message Passing Interface since 1990s -> specification, is implemented in MPICH or Open MPI
    
    

####### MPI insights
- Before 1994: Ḿessage Passing Model, e.g. the master process gives the worker process a message describing the work. The worker process can give back a message that contains the results.
- At Supercomputing 1992 conference, ppl gathered and tried to unify the models, turned out to be MPI-1 (definitions for the interface), launched 1994 and implemented within one year.
- Communicator: defines a group of processes that have the ability to communicate with each other
    - Each process has a unique rank which is explicitely used for that communication.
- Communication = send and receive operations
    - A message can be sent by providing the rank of the receiving process and a unique tag to identify the message 
    - Receiver: posts a receive for a message 
    - 1 sender, 1 receiver => point-to-point communication
- collective communication: e.g. 1 master process needs to broadcast information to all its worker processes
- Sending in more detail: Process A wants to send to Process B
    - A packs up all relevant data into a buffer (envelope) for B
    - Then, the communication device (often a network) routes messsage to proper location (defined by the process's rank)
    - B then has to acknowledge that it is willing to receive A's data, only then the data is transmitted, 
    - A is acknowledged once data is transmitted and can go back to work.
- Tags: Sometimes messages are specified by their ID, which is then their tag. 
- MPI uses specific datatypes in order to better allocate resources. MPI_xx 


- uses LIS = language-independent specification 
- shared memory: not supported since always, but now there are usually concepts available



Words that should be understood (MPI Vocab)
- communicator: object that is used to define how processes communicate with each other -> it is used to send messages between all running MPI processes.
- barrier
- colors and keys
- calling process
- rank of a calling process -> Master process has rank 0, worker processes have higher numbers as ranks

MPI:comm-split: http://mpitutorial.com/tutorials/introduction-to-groups-and-communicators/
When you only want to talk to a subset of processes, you might want to have more communicators. This function allows you to create one by splitting you communicators



#Investigation of the MPI part of our program

In the functions before the main function, we have the following MPI calls:

MPI:gatherv-f64vector
genpoint-projection
point-projection

MPI:barrier my-comm are in the following routines:
GC-GoC-connections   -> genpoint-projection
GoC-GoC-connections  -> point-projection
GoC-GoC-gap-connections -> point-projection
Here, they are called before and after one of the projection methods with the gatherv commands are invoked

In the main function:
MPI:init
    initializes MPI execution environment and creates default communicator. Must be called before any other MPI routine
    Then in the first big let- statement, we find
MPI:get-comm-world

MPI:comm-rank comm-world
    -> processes have ranks. The process with rank 0 is the master process, all other processes are called worker processes and have a higher rank.
MPI:comm-split comm-world color myrank 
    -> apparently this defines whether and into how many sections the stuff should be splitted.
MPI:comm-size my-comm
    -> gives the number of runnning MPI processes, including the current one

MPI:barrier comm-world
Then, the connections methods from before are called one after the other, and after each, the barrier command is repeated

MPI:finalize





####### Ivan's mail: insights

Places to find code:
- check out granular mode repo
- should be called something like PFprojection.scm
- Ivan's home directory on the OIST cluster (if still existent) -> src subdirectory

Hope on the deploy dilemma:
- http://wiki.call-cc.org/nemo
- check out the old deploy script

Conceptual summary:
- construct lines that represent the PFs and GoC dendrites
- sample lines and obtian point clouds
- put it all in a KD- tree
- do parallel KD tree queries to determine the distances between PF points and GoC points (probably this part might be the conceptually hardest, MPI involved)

-the bspline library needs the Fortran code.
- Ivan uses openMPI 1.10.2
mpirun -np 8 $HOME/bin/brep/brep
--rng-seeds=""73,79,83,89,97,101,103,107,109,113""
--gc-points=GCcoordinates.sorted.dat
--gct-points=GCTcoordinates.sorted.dat
--goc-points=GoCcoordinates.sorted.dat --config-file=Parameters.hoc
-:hm16000M






###### Chicken Scheme Profiler
http://www.more-magic.net/posts/statistical-profiling.html
insights on the way
- call history of a scheme error: Shows a trace of the execution flow that led to the error.
 It shows you the line in which the function was called, i.e. if there are different lines, the function was called in different lines (independent from where the function was defined)
 - Usually, scheme supports first-class continuation, and stores trace points that can tell you which function was invoked when. However, if you add -d0 or -no-trace to the compiling process, this trace will be omitted.


Reification: turn an idea about a computer program into an explicit data model or other in a programming language and make it thus available for formal manipulation. 

Continuation: abstract representation of the control state of a computer program. -> with first-class continuation, the execution state of the program is saved with it. 



###### Jupyter notebook cool shortcuts

Scratchpad:     Ctrl + B
Fold/Unfold Code: Ctrl + F
2 to 3 converter: Ctrl + M

Interesting extensions that were not yet enabled: Snippets
/




I had a few first insights about GPU computing and I played around a little bit with PyOpenCL, however that is a while ago and did not lead to any real results. I have little experience with C++ and even less with C, but I know Python and MATLAB. 


I think GPU computing is highly relevant for anyone who works on big simulations or certain ML tasks (like me), and I am thinking about attempting to get my current project to run on a GPU.

If you think my lack of C++/C skills will be a big problem and I might not be able to follow the course, please let me know!